{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import lru_cache, reduce\n",
    "import math\n",
    "from typing import Union\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch._dynamo\n",
    "from torch.nn import functional as F\n",
    "\n",
    "try:\n",
    "    import natten\n",
    "except ImportError:\n",
    "    natten = None\n",
    "\n",
    "try:\n",
    "    import flash_attn\n",
    "except ImportError:\n",
    "    flash_attn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearGEGLU(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features * 2, bias=bias)\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x @ weight.mT\n",
    "        if bias is not None:\n",
    "            x = x + bias\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "class AdaRMSNorm(nn.Module):\n",
    "    def __init__(self, features, cond_features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.linear = nn.Linear(cond_features, features, bias=False)\n",
    "        self.init_parameter()\n",
    "\n",
    "    def init_parameter(self):\n",
    "        nn.init.zeros_(self.linear.weight)\n",
    "        if self.linear.bias is not None:\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        \"\"\"\n",
    "        x: (B, n_heads, tile_H*tile_W, d_head) \n",
    "        \"\"\"\n",
    "        scale = self.linear(cond)[:, None, None, :] + 1\n",
    "\n",
    "        dtype = reduce(torch.promote_types, (x.dtype, scale.dtype, torch.float32))\n",
    "        mean_sq = torch.mean(x.to(dtype)**2, dim=-1, keepdim=True)\n",
    "        scale = scale.to(dtype) * torch.rsqrt(mean_sq + self.eps) \n",
    "\n",
    "        return x * scale.to(x.dtype)\n",
    "\n",
    "class AxialRoPE(nn.Module):\n",
    "    def __init__(self, dim, n_heads):\n",
    "        super().__init__()\n",
    "        log_min = math.log(math.pi)\n",
    "        log_max = math.log(10.0 * math.pi)\n",
    "        freqs = torch.linspace(log_min, log_max, n_heads * dim // 4 + 1)[:-1].exp() \n",
    "        self.register_buffer(\"freqs\", freqs.view(dim // 4, n_heads).T.contiguous()) # freqs는 학습하지 않는 데이터\n",
    "\n",
    "    def forward(self, pos):\n",
    "        # pos : (tile_H*tile_W, 2) 타일 좌표값\n",
    "        # self.freq: (n_heads, dim//4)\n",
    "\n",
    "        # (tile_H*tile_W, n_heads, dim//4)\n",
    "        theta_h = pos[..., None, 0:1] * self.freqs.to(pos.dtype) # 행 좌표값에 freq 곱\n",
    "        theta_w = pos[..., None, 1:2] * self.freqs.to(pos.dtype) # 열 좌표값에 freq 곱\n",
    "\n",
    "        return torch.cat((theta_h, theta_w), dim=-1) # (tile_H*tile_W, n_heads, dim//2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_rotary_emb_inplace(x, theta, conj):\n",
    "    dtype = reduce(torch.promote_types, (x.dtype, theta.dtype, torch.float32))\n",
    "    d = theta.shape[-1]\n",
    "    assert d * 2 <= x.shape[-1]\n",
    "    x1, x2 = x[..., :d], x[..., d : d * 2]\n",
    "    x1_, x2_, theta = x1.to(dtype), x2.to(dtype), theta.to(dtype)\n",
    "    cos, sin = torch.cos(theta), torch.sin(theta)\n",
    "    sin = -sin if conj else sin\n",
    "    y1 = x1_ * cos - x2_ * sin\n",
    "    y2 = x2_ * cos + x1_ * sin\n",
    "    x1.copy_(y1)\n",
    "    x2.copy_(y2)\n",
    "\n",
    "\n",
    "class ApplyRotaryEmbeddingInplace(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x, theta, conj):\n",
    "        _apply_rotary_emb_inplace(x, theta, conj=conj)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        _, theta, conj = inputs\n",
    "        ctx.save_for_backward(theta)\n",
    "        ctx.conj = conj\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        theta, = ctx.saved_tensors\n",
    "        _apply_rotary_emb_inplace(grad_output, theta, conj=not ctx.conj)\n",
    "        return grad_output, None, None\n",
    "\n",
    "\n",
    "def apply_rotary_emb_(x, theta):\n",
    "    return ApplyRotaryEmbeddingInplace.apply(x, theta, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_head, cond_features, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "        self.n_heads = d_model // d_head\n",
    "        self.norm = AdaRMSNorm(d_model, cond_features)\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3, bias=False)\n",
    "        self.scale = nn.Parameter(torch.full([self.n_heads], 10.0))\n",
    "        self.pos_emb = AxialRoPE(d_head // 2, self.n_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.init_parameter()\n",
    "\n",
    "    def init_parameter(self):\n",
    "        nn.init.zeros_(self.out_proj.weight)\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.zeros_(self.out_proj.bias) \n",
    "\n",
    "    def scale_for_cosine_sim(self, q, k, scale, eps):\n",
    "        dtype = reduce(torch.promote_types, (q.dtype, k.dtype, scale.dtype, torch.float32))\n",
    "        sum_sq_q = torch.sum(q.to(dtype)**2, dim=-1, keepdim=True)\n",
    "        sum_sq_k = torch.sum(k.to(dtype)**2, dim=-1, keepdim=True)\n",
    "        sqrt_scale = torch.sqrt(scale.to(dtype))\n",
    "        scale_q = sqrt_scale * torch.rsqrt(sum_sq_q + eps)\n",
    "        scale_k = sqrt_scale * torch.rsqrt(sum_sq_k + eps)\n",
    "        return q * scale_q.to(q.dtype), k * scale_k.to(k.dtype)\n",
    "\n",
    "    def forward(self, x, pos, cond):\n",
    "        #x : (B, tile_H, tile_W, d_model)\n",
    "        skip = x\n",
    "        x = self.norm(x, cond) \n",
    "        qkv = self.qkv_proj(x) # (B, tile_H, tile_W, 3*d_model)\n",
    "        pos = rearrange(pos, \"... h w e -> ... (h w) e\").to(qkv.dtype) # tile의 위치정보를 2차원에서 1차원으로\n",
    "        theta = self.pos_emb(pos) # (tile_H*tile_W, n_head, d_head//4)\n",
    "        #theta의 차원이 d_head//4이므로 apply_rotary_emb_가 앞 절반 차원에만 적용된다.\n",
    "\n",
    "        # q,k,v: (B, n_heads, tile_H*tile_W, d_head)\n",
    "        q, k, v = rearrange(qkv, \"n h w (t nh e) -> t n nh (h w) e\", t=3, e=self.d_head) # nh: number of head\n",
    "        q, k = self.scale_for_cosine_sim(q, k, self.scale[:, None, None], 1e-6)\n",
    "        theta = theta.movedim(-2, -3) # (n_head, tile_H*tile_W, d_head//4)\n",
    "        q = apply_rotary_emb_(q, theta)\n",
    "        k = apply_rotary_emb_(k, theta)\n",
    "        x = F.scaled_dot_product_attention(q, k, v, scale=1.0) # (B, n_heads, tile_H*tile_W, d_head)\n",
    "        x = rearrange(x, \"n nh (h w) e -> n h w (nh e)\", h=skip.shape[-3], w=skip.shape[-2]) # (B, tile_H, tile_W, d_model)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x + skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = AxialRoPE(8, 32)\n",
    "pos = rearrange(make_axial_pos(2, 4).view(2, 4, 2), \"... h w e -> ... (h w) e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 4])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(pos).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 16])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.randn(10, 8, 16, 32), dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box(h, w, pixel_aspect_ratio=1.0):\n",
    "    # Adjusted dimensions\n",
    "    w_adj = w\n",
    "    h_adj = h * pixel_aspect_ratio\n",
    "\n",
    "    # Adjusted aspect ratio\n",
    "    ar_adj = w_adj / h_adj\n",
    "\n",
    "    # Determine bounding box based on the adjusted aspect ratio\n",
    "    y_min, y_max, x_min, x_max = -1.0, 1.0, -1.0, 1.0\n",
    "    if ar_adj > 1:\n",
    "        y_min, y_max = -1 / ar_adj, 1 / ar_adj\n",
    "    elif ar_adj < 1:\n",
    "        x_min, x_max = -ar_adj, ar_adj\n",
    "\n",
    "    return y_min, y_max, x_min, x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 0.5, -1.0, 1.0)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_box(30, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(h_pos, w_pos):\n",
    "    grid = torch.stack(torch.meshgrid(h_pos, w_pos, indexing='ij'), dim=-1)\n",
    "    h, w, d = grid.shape # (h, w)에 해당하는 변환된 d dim의 좌표\n",
    "    return grid.view(h * w, d)\n",
    "\n",
    "\n",
    "def bounding_box(h, w, pixel_aspect_ratio=1.0):\n",
    "    '''\n",
    "    긴 축을 -1 ~ 1 로 변환했을 때의 최소/최댓값\n",
    "    h = 30, w = 60일 때 (-0.5, 0.5, -1.0, 1.0)\n",
    "    '''\n",
    "\n",
    "    # Adjusted dimensions\n",
    "    w_adj = w\n",
    "    h_adj = h * pixel_aspect_ratio\n",
    "\n",
    "    # Adjusted aspect ratio\n",
    "    ar_adj = w_adj / h_adj\n",
    "\n",
    "    # Determine bounding box based on the adjusted aspect ratio\n",
    "    y_min, y_max, x_min, x_max = -1.0, 1.0, -1.0, 1.0\n",
    "    if ar_adj > 1:\n",
    "        y_min, y_max = -1 / ar_adj, 1 / ar_adj\n",
    "    elif ar_adj < 1:\n",
    "        x_min, x_max = -ar_adj, ar_adj\n",
    "\n",
    "    return y_min, y_max, x_min, x_max\n",
    "\n",
    "\n",
    "def make_axial_pos(h, w, pixel_aspect_ratio=1.0, align_corners=False, dtype=None, device=None):\n",
    "    y_min, y_max, x_min, x_max = bounding_box(h, w, pixel_aspect_ratio)\n",
    "    if align_corners:\n",
    "        h_pos = torch.linspace(y_min, y_max, h, dtype=dtype, device=device)\n",
    "        w_pos = torch.linspace(x_min, x_max, w, dtype=dtype, device=device)\n",
    "    else:\n",
    "        # 중앙 정렬된 h, w개수 만큼 bounding box 내의 값을 나눔\n",
    "        h_pos = centers(y_min, y_max, h, dtype=dtype, device=device)\n",
    "        w_pos = centers(x_min, x_max, w, dtype=dtype, device=device)\n",
    "    return make_grid(h_pos, w_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2500, -0.7500],\n",
       "        [-0.2500, -0.2500],\n",
       "        [-0.2500,  0.2500],\n",
       "        [-0.2500,  0.7500],\n",
       "        [ 0.2500, -0.7500],\n",
       "        [ 0.2500, -0.2500],\n",
       "        [ 0.2500,  0.2500],\n",
       "        [ 0.2500,  0.7500]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_axial_pos(2, 4) # (B, 2, 4, C)의 데이터일때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborhoodSelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_head, cond_features, kernel_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "        self.n_heads = d_model // d_head\n",
    "        self.kernel_size = kernel_size\n",
    "        self.norm = AdaRMSNorm(d_model, cond_features)\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3, bias=False)\n",
    "        self.scale = nn.Parameter(torch.full([self.n_heads], 10.0))\n",
    "        self.pos_emb = AxialRoPE(d_head // 2, self.n_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.init_parameter()\n",
    "\n",
    "    def init_parameter(self):\n",
    "        nn.init.zeros_(self.out_proj.weight)\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.zeros_(self.out_proj.bias) \n",
    "\n",
    "    def scale_for_cosine_sim(self, q, k, scale, eps):\n",
    "        dtype = reduce(torch.promote_types, (q.dtype, k.dtype, scale.dtype, torch.float32))\n",
    "        sum_sq_q = torch.sum(q.to(dtype)**2, dim=-1, keepdim=True)\n",
    "        sum_sq_k = torch.sum(k.to(dtype)**2, dim=-1, keepdim=True)\n",
    "        sqrt_scale = torch.sqrt(scale.to(dtype))\n",
    "        scale_q = sqrt_scale * torch.rsqrt(sum_sq_q + eps)\n",
    "        scale_k = sqrt_scale * torch.rsqrt(sum_sq_k + eps)\n",
    "        return q * scale_q.to(q.dtype), k * scale_k.to(k.dtype)\n",
    "\n",
    "    def forward(self, x, pos, cond):\n",
    "        skip = x\n",
    "        x = self.norm(x, cond)\n",
    "        qkv = self.qkv_proj(x)\n",
    "        if natten is None:\n",
    "            raise ModuleNotFoundError(\"natten is required for neighborhood attention\")\n",
    "\n",
    "        q, k, v = rearrange(qkv, \"n h w (t nh e) -> t n nh h w e\", t=3, e=self.d_head)\n",
    "        q, k = scale_for_cosine_sim(q, k, self.scale[:, None, None, None], 1e-6)\n",
    "        theta = self.pos_emb(pos).movedim(-2, -4)\n",
    "        q = apply_rotary_emb_(q, theta)\n",
    "        k = apply_rotary_emb_(k, theta)\n",
    "        qk = natten.functional.na2d_qk(q, k, self.kernel_size)\n",
    "        a = torch.softmax(qk, dim=-1).to(v.dtype)\n",
    "        x = natten.functional.na2d_av(a, v, self.kernel_size)\n",
    "        x = rearrange(x, \"n nh h w e -> n h w (nh e)\")\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x + skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "scaled_dot_product_attention() got an unexpected keyword argument 'scale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m down \u001b[38;5;241m=\u001b[39m SelfAttentionBlock(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      2\u001b[0m test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[169], line 42\u001b[0m, in \u001b[0;36mSelfAttentionBlock.forward\u001b[0;34m(self, x, pos, cond)\u001b[0m\n\u001b[1;32m     40\u001b[0m q \u001b[38;5;241m=\u001b[39m apply_rotary_emb_(q, theta)\n\u001b[1;32m     41\u001b[0m k \u001b[38;5;241m=\u001b[39m apply_rotary_emb_(k, theta)\n\u001b[0;32m---> 42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn nh (h w) e -> n h w (nh e)\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39mskip\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], w\u001b[38;5;241m=\u001b[39mskip\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "\u001b[0;31mTypeError\u001b[0m: scaled_dot_product_attention() got an unexpected keyword argument 'scale'"
     ]
    }
   ],
   "source": [
    "down = SelfAttentionBlock(64, 8, 64)\n",
    "test = torch.randn(10, 4, 4, 64)\n",
    "down(test, torch.randn(4, 4, 2), torch.randn(10, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4, 2])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((1,2,3,4)).movedim(-3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearGEGLU(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features * 2, bias=bias)\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x @ weight.mT\n",
    "        if bias is not None:\n",
    "            x = x + bias\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, cond_features, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm = AdaRMSNorm(d_model, cond_features)\n",
    "        self.up_proj = LinearGEGLU(d_model, d_ff, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
    "        self.init_parameter()\n",
    "\n",
    "    def init_parameter(self):\n",
    "        nn.init.zeros_(self.down_proj.weight)\n",
    "        if self.down_proj.bias is not None:\n",
    "            nn.init.zeros_(self.down_proj.bias) \n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        skip = x\n",
    "        x = self.norm(x, cond)\n",
    "        x = self.up_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.down_proj(x)\n",
    "        return x + skip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
